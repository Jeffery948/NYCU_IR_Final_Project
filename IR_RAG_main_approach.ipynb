{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 下載 litqa-v0.jsonl"
      ],
      "metadata": {
        "id": "JMUT-b04s5yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Future-House/LitQA/main/litqa-v0.jsonl"
      ],
      "metadata": {
        "id": "ADtbjU4ps2uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 下載 FAISS_db.zip"
      ],
      "metadata": {
        "id": "nPbl8tM94mQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://drive.google.com/uc?export=download&id=1fQSmLJxOmGmHYlqUUamZKGIm4w1JmEXI\"\n",
        "!curl -L -o FAISS_db.zip \"$url\""
      ],
      "metadata": {
        "id": "dj5SwSU64s2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip FAISS_db.zip"
      ],
      "metadata": {
        "id": "v-67llwY5v8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 安裝並引入必要套件"
      ],
      "metadata": {
        "id": "R31bNOZu52eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community pypdf python-docx sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "id": "p8lNFOuR53uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import json\n",
        "import os\n",
        "import csv\n",
        "import re\n",
        "import time  # 新增 time 模組用於等待\n",
        "from typing import List, Optional\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1C4MRG26ADZ",
        "outputId": "dff9bce7-3166-4ad4-eed5-559647faf7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自訂 E5 embedding 類別"
      ],
      "metadata": {
        "id": "Vf2yiggo57xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomE5Embedding(HuggingFaceEmbeddings):\n",
        "    def embed_documents(self, texts):\n",
        "        texts = [f\"passage: {t}\" for t in texts]\n",
        "        return super().embed_documents(texts)\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return super().embed_query(f\"query: {text}\")"
      ],
      "metadata": {
        "id": "GP5cSWnX6Djm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 載入 `faiss_db`"
      ],
      "metadata": {
        "id": "awQLxo0j6MAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = CustomE5Embedding(model_name=\"intfloat/multilingual-e5-small\")\n",
        "db = FAISS.load_local(\"faiss_db\", embedding_model, allow_dangerous_deserialization=True)\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "W4n7S5hV6UOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 設定 ID → Paper Title"
      ],
      "metadata": {
        "id": "6inTWW_HeZ2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paper_title = [\n",
        "\"Lymph node targeted multi-epitope subunit vaccine promotes effective immunity to EBV in HLA-expressing mice\",\n",
        "\"Fasting mimicking diet in mice delays cancer growth and reduces immunotherapy associated cardiovascular and systemic side effects\",\n",
        "\"Single-cell transcriptome analysis indicates fatty acid metabolism-mediated metastasis and immunosuppression in male breast cancer\",\n",
        "\"Metabolic glycan labeling immobilizes dendritic cell membrane and enhances antitumor efficacy of dendritic cell vaccine\",\n",
        "\"Primary germinal center-resident T follicular helper cells are a physiologically distinct subset of CXCR5hiPD-1hi T follicular helper cells\",\n",
        "\"Two-photon synthetic aperture microscopy for minimally invasive fast 3D imaging of native subcellular behaviors in deep tissue\",\n",
        "\"Single-cell protein expression profiling resolves circulating and resident memory T cell diversity across tissues and infection contexts\",\n",
        "\"Symmetric Molecular Dynamics\",\n",
        "\"Transcriptomic taxonomy and neurogenic trajectories of adult human, macaque, and pig hippocampal and entorhinal cells\",\n",
        "\"Multi-omic analysis reveals divergent molecular events in scarring and regenerative wound healing\",\n",
        "\"Chromatin-state barriers enforce an irreversible mammalian cell fate decision\",\n",
        "\"Prenatal environmental stressors impair postnatal microglia function and adult behavior in males\",\n",
        "\"Prenatal environmental stressors impair postnatal microglia function and adult behavior in males\",\n",
        "\"Evolutionarily conserved bacterial effectors hijack abscisic acid signaling to induce an aqueous environment in the apoplast\",\n",
        "\"Evolutionarily conserved bacterial effectors hijack abscisic acid signaling to induce an aqueous environment in the apoplast\",\n",
        "\"Evolutionarily conserved bacterial effectors hijack abscisic acid signaling to induce an aqueous environment in the apoplast\",\n",
        "\"Enhanced prime editing systems by manipulating cellular determinants of editing outcomes\",\n",
        "\"Intestinal Microbiota Influence Doxorubicin Responsiveness in Triple-Negative Breast Cancer\",\n",
        "\"Deciphering the molecular organization of GET pathway chaperones through native mass spectrometry\",\n",
        "\"Deciphering the molecular organization of GET pathway chaperones through native mass spectrometry\",\n",
        "\"Tertiary lymphoid structures generate and propagate anti-tumor antibody-producing plasma cells in renal cell cancer\",\n",
        "\"Slow and negligible senescence among testudines challenges evolutionary theories of senescence\",\n",
        "\"Thymic epithelial cells co-opt lineage-defining transcription factors to eliminate autoreactive T cells\",\n",
        "\"Peptide-guided lipid nanoparticles deliver mRNA to the neural retina of rodents and nonhuman primates\",\n",
        "\"Massive Multiplexing of Spatially Resolved Single Neuron Projections with Axonal BARseq\",\n",
        "\"Dynamic mapping of proteome trafficking within and between living cells by TransitID\",\n",
        "\"The connectome of an insect brain\",\n",
        "\"The connectome of an insect brain\",\n",
        "\"A transcription factor atlas of directed differentiation\",\n",
        "\"Controlled Protein Activities with Viral Proteases, Antiviral Peptides, and Antiviral Drugs\",\n",
        "\"Spatial imaging of glycoRNA in single cells with ARPLA\",\n",
        "\"Spatial imaging of glycoRNA in single cells with ARPLA\",\n",
        "\"Discovery of new deaminase functions by structure-based protein clustering\",\n",
        "\"Rescue of α-synuclein aggregation in Parkinson’s patient neurons by synergistic enhancement of ER proteostasis and protein trafficking\",\n",
        "\"Discovery of new deaminase functions by structure-based protein clustering\",\n",
        "\"RhoA drives actin compaction to restrict axon regeneration and astrocyte reactivity after CNS injury\",\n",
        "\"Prenatal environmental stressors impair postnatal microglia function and adult behavior in males\",\n",
        "\"Concerted type I interferon signaling in microglia and neural cells promotes memory impairment associated with amyloid β plaques\",\n",
        "\"Concerted type I interferon signaling in microglia and neural cells promotes memory impairment associated with amyloid β plaques\",\n",
        "\"The allergy mediator histamine confers resistance to immunotherapy in cancer patients via activation of the macrophage histamine receptor H1\",\n",
        "\"Conserved cell types with divergent features in human versus mouse cortex\",\n",
        "\"Connectomic comparison of mouse and human cortex\",\n",
        "\"Slide-seq: A scalable technology for measuring genome-wide expression at high spatial resolution\",\n",
        "\"Slide-seq: A scalable technology for measuring genome-wide expression at high spatial resolution\",\n",
        "\"Massively parallel base editing to map variant effects in human hematopoiesis\",\n",
        "\"Scalable full-transcript coverage single cell RNA sequencing with Smart-seq3xpress\",\n",
        "\"Illuminating protein space with a programmable generative mode\",\n",
        "\"Long-term platinum-based drug accumulation in cancer-associated fibroblasts promotes colorectal cancer progression and resistance to therapy\",\n",
        "\"Pan-KRAS inhibitor disables oncogenic signalling and tumour growth\",\n",
        "\"Bempegaldesleukin (NKTR-214): a CD-122-biased IL-2 receptor agonist for cancer immunotherapy\"\n",
        "]"
      ],
      "metadata": {
        "id": "dhIDfAmpepu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(paper_title))"
      ],
      "metadata": {
        "id": "vy7Yj6ltfYWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_title = {}\n",
        "count = 0\n",
        "with open('litqa-v0.jsonl', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        if \"canary\" in data: continue\n",
        "        id_to_title[data['id']] = paper_title[count].lower()\n",
        "        count += 1"
      ],
      "metadata": {
        "id": "AoB4qW5dehpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in id_to_title.items():\n",
        "    print(k, v)\n",
        "print(len(id_to_title))"
      ],
      "metadata": {
        "id": "VRM9NgFugwZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base RAG"
      ],
      "metadata": {
        "id": "NfB703JttAfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k56QJz3KpT_u"
      },
      "outputs": [],
      "source": [
        "# 若未設定環境變數，請在此填入您的 API Key\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_ACTUAL_API_KEY\"\n",
        "\n",
        "class LitQAEvaluator:\n",
        "    def __init__(self, model_name: str = \"gemini-2.5-flash-lite\"):\n",
        "        self.client = genai.Client(api_key=userdata.get('Gemini'))\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def _get_system_prompt(self, has_references: bool) -> str:\n",
        "        \"\"\"OpenScholar 風格 System Prompt\"\"\"\n",
        "        base_prompt = (\n",
        "            \"Provide a detailed, informative answer to the following research-related question. Your answer should be more than one paragraph. \"\n",
        "            \"Base your answer on multiple pieces of evidence. \"\n",
        "            \"Make sure to add citations to all citation-worthy statements using reference numbers (e.g., [1], [2]). \"\n",
        "        )\n",
        "\n",
        "        if has_references:\n",
        "            ref_instruction = \"Use the provided 'References' section below. Add the citation number at the end of each relevant sentence. \"\n",
        "        else:\n",
        "            ref_instruction = (\n",
        "                \"NO external references are provided. You must strictly use your INTERNAL KNOWLEDGE to identify and cite real, existing scientific papers. \"\n",
        "                \"Generate citations numbers [1], [2]... in the text and list the full Paper Titles at the end.\"\n",
        "            )\n",
        "\n",
        "        output_instruction = (\n",
        "            \"You MUST produce output in **exactly this structure**:\\n\"\n",
        "            \"[Response_Start]\\n\"\n",
        "            \"Your reasoning here.\\n\"\n",
        "            \"\\n\"\n",
        "            \"[Response_End]\\n\"\n",
        "            \"{\\\"ans\\\": \\\"A\\\"}\\n\"\n",
        "            \"References:\\n\"\n",
        "            \"[1] <Most relevant document title>\\n\"\n",
        "            \"[2] <Second relevant document title>\\n\"\n",
        "            \"[3] <Third relevant document title>\\n\"\n",
        "            \"\\n\"\n",
        "            \"Rules:\\n\"\n",
        "            \"- This is a Multiple Choice Question. Select the most accurate option based on the reasoning.\\n\"\n",
        "            \"- You may include multiple references, numbered [1], [2], [3], ...\\n\"\n",
        "            \"- Each reference must correspond to a real, relevant document.\\n\"\n",
        "            \"- Do NOT fabricate titles; use exact titles from provided documents when possible.\\n\"\n",
        "            \"- If provided documents are not relevant, you may cite real papers from your internal knowledge instead.\\n\"\n",
        "            \"- Do NOT repeat the same title.\\n\"\n",
        "            \"- The JSON (e.g., {\\\"ans\\\": \\\"A\\\"}) must appear **immediately after [Response_End]** and be the ONLY JSON object.\\n\"\n",
        "        )\n",
        "\n",
        "        return base_prompt + ref_instruction + output_instruction\n",
        "\n",
        "    def format_question_data(self, data: dict) -> tuple:\n",
        "        \"\"\"整理問題與選項，計算正確答案代號 (A, B, C...)\"\"\"\n",
        "        question_text = data[\"question\"]\n",
        "        ideal = data[\"ideal\"]\n",
        "\n",
        "        all_options = sorted([ideal] + data[\"distractors\"])\n",
        "        correct_index = all_options.index(ideal)\n",
        "        correct_letter = chr(65 + correct_index)\n",
        "\n",
        "        options_str = \"\"\n",
        "        for idx, opt in enumerate(all_options):\n",
        "            options_str += f\"{chr(65 + idx)}. {opt}\\n\"\n",
        "\n",
        "        return question_text, options_str, correct_letter, all_options\n",
        "\n",
        "    def extract_model_answer(self, text: str) -> str:\n",
        "        \"\"\"使用 Regex 提取 {\"ans\": \"X\"}\"\"\"\n",
        "        match = re.search(r'\\{\\s*\"ans\"\\s*:\\s*\"([A-Z])\"\\s*\\}', text, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1).upper()\n",
        "        return \"PARSE_ERROR\"\n",
        "\n",
        "    def evaluate_references(self, output_text: str, id: str) -> bool:\n",
        "        \"\"\"檢查模型輸出是否包含正確的 Reference 來源\"\"\"\n",
        "        output_lower = output_text.lower()\n",
        "        #print(output_lower)\n",
        "        #print(id_to_title[id])\n",
        "        if id_to_title[id] in output_lower:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def run_evaluation(self, input_file: str, output_file: str, retrieve=False):\n",
        "        \"\"\"執行評測，包含斷點續傳與 API 重試機制\"\"\"\n",
        "        fieldnames = [\n",
        "            \"id\", \"question\", \"correct_answer\", \"predicted_answer\",\n",
        "            \"is_answer_correct\", \"is_reference_correct\", \"full_output\"\n",
        "        ]\n",
        "\n",
        "        # --- 1. 斷點續傳邏輯 ---\n",
        "        processed_ids = set()\n",
        "        file_exists = os.path.exists(output_file) and os.path.getsize(output_file) > 0\n",
        "\n",
        "        if file_exists:\n",
        "            print(f\"Output file '{output_file}' found. Reading processed IDs for resuming...\")\n",
        "            with open(output_file, 'r', encoding='utf-8') as f_read:\n",
        "                reader = csv.DictReader(f_read)\n",
        "                for row in reader:\n",
        "                    if \"id\" in row and row[\"id\"]:\n",
        "                        processed_ids.add(row[\"id\"])\n",
        "            print(f\"-> Found {len(processed_ids)} processed items. Skipping them.\")\n",
        "        else:\n",
        "            print(f\"-> Starting new evaluation. Output will be saved to '{output_file}'.\")\n",
        "\n",
        "        ans_correct_total = 0\n",
        "        ref_correct_total = 0\n",
        "\n",
        "        # --- 2. 開啟檔案 (Append 模式) ---\n",
        "        with open(input_file, 'r', encoding='utf-8') as f_in, \\\n",
        "             open(output_file, 'a', newline='', encoding='utf-8') as f_out:\n",
        "\n",
        "            writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
        "            if not file_exists:\n",
        "                writer.writeheader()\n",
        "\n",
        "            count = 1\n",
        "            for line_idx, line in enumerate(f_in):\n",
        "                line = line.strip()\n",
        "                if not line: continue\n",
        "\n",
        "                try:\n",
        "                    row_data = json.loads(line)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Skipping line {line_idx+1}: Invalid JSON.\")\n",
        "                    continue\n",
        "\n",
        "                if \"canary\" in row_data: continue\n",
        "\n",
        "                curr_id = row_data.get(\"id\")\n",
        "\n",
        "                # --- 3. 跳過已處理題目 ---\n",
        "                if curr_id in processed_ids:\n",
        "                    continue\n",
        "\n",
        "                # --- 4. 準備 Input ---\n",
        "                q_text, opt_str, correct_letter, _ = self.format_question_data(row_data)\n",
        "\n",
        "                has_refs = False\n",
        "                ref_context = \"\"\n",
        "\n",
        "                if retrieve:\n",
        "                    try:\n",
        "                        all_docs = retriever.invoke(q_text)\n",
        "                        docs = [d.page_content for d in all_docs]\n",
        "                        if docs:\n",
        "                            has_refs = True\n",
        "                            ref_context = \"References:\\n\" + \"\\n\".join([f\"[{i+1}] {d}\" for i, d in enumerate(docs)])\n",
        "                            #print(f\"Retrieved {len(docs)} references for ID {curr_id}.\")\n",
        "                            #print(f\"Check docs: {ref_context}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Retriever Error for {curr_id}: {e}\")\n",
        "\n",
        "                system_instruction = self._get_system_prompt(has_refs)\n",
        "                full_content = (\n",
        "                    f\"{ref_context}\\n\"\n",
        "                    f\"Question: {q_text}\\n\"\n",
        "                    f\"Options:\\n{opt_str}\\n\"\n",
        "                    f\"Now, please answer this question following the system instructions.\\n\"\n",
        "                )\n",
        "\n",
        "                print(f\"#{count} Processing ID: {curr_id} ... \", end=\"\", flush=True)\n",
        "                count += 1\n",
        "\n",
        "                # --- 5. 呼叫 Gemini API (含重試機制) ---\n",
        "                output_text = None\n",
        "                max_retries = 5\n",
        "                base_wait_time = 20  # 基礎等待時間 20 秒\n",
        "\n",
        "                for attempt in range(max_retries + 1):\n",
        "                    try:\n",
        "                        response = self.client.models.generate_content(\n",
        "                            model=self.model_name,\n",
        "                            contents=full_content,\n",
        "                            config=types.GenerateContentConfig(\n",
        "                                system_instruction=system_instruction,\n",
        "                                temperature=0.0\n",
        "                            )\n",
        "                        )\n",
        "                        output_text = response.text\n",
        "                        break  # 成功，跳出重試迴圈\n",
        "\n",
        "                    except Exception as e:\n",
        "                        if attempt < max_retries:\n",
        "                            # 計算等待時間: 20 * (2^0), 20 * (2^1)... -> 20, 40, 80, 160, 320\n",
        "                            wait_time = base_wait_time * (2 ** attempt)\n",
        "                            print(f\"\\n[API Error] Attempt {attempt+1}/{max_retries+1} failed: {e}\")\n",
        "                            print(f\"-> Retrying in {wait_time} seconds...\")\n",
        "                            time.sleep(wait_time)\n",
        "                        else:\n",
        "                            print(f\"\\n[API Failed] All {max_retries+1} attempts failed. Error: {e}\")\n",
        "                            output_text = None # 標記為失敗\n",
        "\n",
        "                # 如果最後還是失敗 (output_text 為 None)，跳過此題，不寫入 CSV，以便下次重跑\n",
        "                if output_text is None:\n",
        "                    print(f\"Skipping ID {curr_id} due to API failures.\")\n",
        "                    continue\n",
        "\n",
        "                # --- 6. 評估與記錄 ---\n",
        "                predicted_ans = self.extract_model_answer(output_text)\n",
        "                is_ans_correct = (predicted_ans == correct_letter)\n",
        "\n",
        "                is_ref_correct = self.evaluate_references(output_text, curr_id)\n",
        "\n",
        "                ans_correct_total += is_ans_correct\n",
        "                ref_correct_total += is_ref_correct\n",
        "\n",
        "                print(f\"Ans: {predicted_ans} (Correct: {correct_letter}) | Ref Valid: {is_ref_correct}\")\n",
        "\n",
        "                writer.writerow({\n",
        "                    \"id\": curr_id,\n",
        "                    \"question\": q_text,\n",
        "                    \"correct_answer\": correct_letter,\n",
        "                    \"predicted_answer\": predicted_ans,\n",
        "                    \"is_answer_correct\": is_ans_correct,\n",
        "                    \"is_reference_correct\": is_ref_correct,\n",
        "                    \"full_output\": output_text\n",
        "                })\n",
        "\n",
        "                # --- 7. 強制寫入硬碟 ---\n",
        "                f_out.flush()\n",
        "\n",
        "        print(f\"\\nEvaluation complete.\")\n",
        "        print(f\"Correct Answers: {ans_correct_total}\")\n",
        "        print(f\"Correct References: {ref_correct_total}\")\n",
        "        print(f\"Accuracy: {ans_correct_total / line_idx * 100:.2f}%\")\n",
        "        print(f\"Reference Accuracy: {ref_correct_total / line_idx * 100:.2f}%\")\n",
        "\n",
        "# --- 主程式執行區塊 ---\n",
        "if __name__ == \"__main__\":\n",
        "    input_filename = \"litqa-v0.jsonl\"\n",
        "    output_filename = \"evaluation.csv\"\n",
        "\n",
        "    if os.path.exists(output_filename): # For testing\n",
        "        os.remove(output_filename)\n",
        "\n",
        "    if not os.path.exists(input_filename):\n",
        "        print(f\"Error: Input file '{input_filename}' not found.\")\n",
        "    else:\n",
        "        evaluator = LitQAEvaluator()\n",
        "        # retriever_func 設為 None，讓 LLM 自行判斷\n",
        "        evaluator.run_evaluation(input_filename, output_filename, retrieve=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main approach"
      ],
      "metadata": {
        "id": "gxROiNWWt4r6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReflectRAGEvaluator:\n",
        "    def __init__(self, model_name: str = \"gemini-2.5-pro\"):\n",
        "        self.client = genai.Client(api_key=userdata.get('Gemini'))\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def format_question_data(self, data: dict) -> tuple:\n",
        "        \"\"\"整理問題與選項，計算正確答案代號 (A, B, C...)\"\"\"\n",
        "        question_text = data[\"question\"]\n",
        "        ideal = data[\"ideal\"]\n",
        "\n",
        "        all_options = sorted([ideal] + data[\"distractors\"])\n",
        "        correct_index = all_options.index(ideal)\n",
        "        correct_letter = chr(65 + correct_index)\n",
        "\n",
        "        options_str = \"\"\n",
        "        for idx, opt in enumerate(all_options):\n",
        "            options_str += f\"{chr(65 + idx)}. {opt}\\n\"\n",
        "\n",
        "        return question_text, options_str, correct_letter, all_options\n",
        "\n",
        "    # =========================================================\n",
        "    # Stage 1: Document-level confidence reasoning\n",
        "    # =========================================================\n",
        "    def get_doc_confidences(self, query: str, docs: List[str]) -> List[dict]:\n",
        "        \"\"\"讓 LLM 為每篇文件生成暫時答案與信心分數\"\"\"\n",
        "        doc_results = []\n",
        "        for i, doc in enumerate(docs):\n",
        "            prompt = f\"\"\"\n",
        "You are a scientific assistant working on literature-based QA.\n",
        "\n",
        "Question: {query}\n",
        "Document [{i+1}]:\n",
        "{doc}\n",
        "\n",
        "Task:\n",
        "1. Based on this document only, answer the question as best as you can.\n",
        "2. After answering, rate your confidence (0–100%) that this document directly supports your answer.\n",
        "\n",
        "Format:\n",
        "Answer: <text>\n",
        "Confidence: <number between 0 and 100>\n",
        "\"\"\"\n",
        "            try:\n",
        "                response = self.client.models.generate_content(\n",
        "                    model=self.model_name,\n",
        "                    contents=prompt,\n",
        "                    config=types.GenerateContentConfig(temperature=0.0)\n",
        "                )\n",
        "                text = response.text\n",
        "                ans_match = re.search(r'Answer:\\s*(.*)', text)\n",
        "                conf_match = re.search(r'Confidence:\\s*(\\d+)', text)\n",
        "                answer = ans_match.group(1).strip() if ans_match else \"N/A\"\n",
        "                conf = int(conf_match.group(1)) if conf_match else 0\n",
        "                doc_results.append({\n",
        "                    \"doc_id\": i + 1,\n",
        "                    \"answer\": answer,\n",
        "                    \"confidence\": conf\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Confidence stage error on doc {i+1}: {e}\")\n",
        "        return doc_results\n",
        "\n",
        "    # =========================================================\n",
        "    # Stage 2: Reflection and evidence consolidation\n",
        "    # =========================================================\n",
        "    def reflect_and_verify(self, query: str, doc_results: List[dict], docs: List[str]) -> str:\n",
        "        \"\"\"反思階段：重新整理 reasoning 與 citation\"\"\"\n",
        "        summary_text = \"\\n\".join(\n",
        "            [f\"[Doc {r['doc_id']}] Answer: {r['answer']} | Confidence: {r['confidence']}%\" for r in doc_results]\n",
        "        )\n",
        "        context_text = \"\\n\".join([f\"[{i+1}] {docs[i]}\" for i in range(len(docs))])\n",
        "\n",
        "        reflection_prompt = f\"\"\"\n",
        "You are a scientific QA expert. You have read multiple papers and produced preliminary answers:\n",
        "\n",
        "{summary_text}\n",
        "\n",
        "Now reflect on your reasoning:\n",
        "1. Re-evaluate which documents actually contain evidence supporting the answer.\n",
        "2. Merge consistent reasoning from multiple supporting documents.\n",
        "3. Identify any documents that were overconfident or irrelevant.\n",
        "4. Provide a concise reasoning summary and list the document IDs that truly support the final conclusion.\n",
        "\n",
        "References:\n",
        "{context_text}\n",
        "\n",
        "Output format:\n",
        "Reflection Summary: <your reasoning>\n",
        "Supporting Citations: [IDs, e.g., 1, 3, 5]\n",
        "\"\"\"\n",
        "        response = self.client.models.generate_content(\n",
        "            model=self.model_name,\n",
        "            contents=reflection_prompt,\n",
        "            config=types.GenerateContentConfig(temperature=0.0)\n",
        "        )\n",
        "        return response.text\n",
        "\n",
        "    # =========================================================\n",
        "    # Stage 3: Final generation (answer + citations)\n",
        "    # =========================================================\n",
        "    def generate_final_answer(self, query: str, option, reflection_output: str) -> str:\n",
        "        final_prompt = f\"\"\"\n",
        "You are writing the final answer for a scientific question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Options: {option}\n",
        "\n",
        "You have already reflected and identified supporting evidence below:\n",
        "{reflection_output}\n",
        "\n",
        "If all retrieved documents are irrelevant, you must strictly use your INTERNAL KNOWLEDGE to identify and cite real, existing scientific papers.\n",
        "\"\"\"\n",
        "\n",
        "        output_instruction = (\n",
        "            \"You MUST produce output in **exactly this structure**:\\n\"\n",
        "            \"[Response_Start]\\n\"\n",
        "            \"Your reasoning here.\\n\"\n",
        "            \"\\n\"\n",
        "            \"[Response_End]\\n\"\n",
        "            \"{\\\"ans\\\": \\\"A\\\"}\\n\"\n",
        "            \"References:\\n\"\n",
        "            \"[1] <Most relevant document title>\\n\"\n",
        "            \"[2] <Second relevant document title>\\n\"\n",
        "            \"[3] <Third relevant document title>\\n\"\n",
        "            \"\\n\"\n",
        "            \"Rules:\\n\"\n",
        "            \"- This is a Multiple Choice Question. Select the most accurate option based on the reasoning.\\n\"\n",
        "            \"- You may include multiple references, numbered [1], [2], [3], ...\\n\"\n",
        "            \"- Each reference must correspond to a real, relevant document.\\n\"\n",
        "            \"- Do NOT fabricate titles; use exact titles from provided documents when possible.\\n\"\n",
        "            \"- If provided documents are not relevant, you may cite real papers from your internal knowledge instead.\\n\"\n",
        "            \"- Do NOT repeat the same title.\\n\"\n",
        "            \"- The JSON (e.g., {\\\"ans\\\": \\\"A\\\"}) must appear **immediately after [Response_End]** and be the ONLY JSON object.\\n\"\n",
        "        )\n",
        "\n",
        "        response = self.client.models.generate_content(\n",
        "            model=self.model_name,\n",
        "            contents=final_prompt + output_instruction,\n",
        "            config=types.GenerateContentConfig(temperature=0.0)\n",
        "        )\n",
        "        return response.text\n",
        "\n",
        "    # =========================================================\n",
        "    # Utility: Extract answer letter\n",
        "    # =========================================================\n",
        "    def extract_model_answer(self, text: str) -> str:\n",
        "        match = re.search(r'\\{\\s*\"ans\"\\s*:\\s*\"([A-Z])\"\\s*\\}', text, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1).upper()\n",
        "        return \"PARSE_ERROR\"\n",
        "\n",
        "    def evaluate_references(self, output_text: str, id: str) -> bool:\n",
        "        \"\"\"檢查模型輸出是否包含正確的 Reference 來源\"\"\"\n",
        "        output_lower = output_text.lower()\n",
        "        if id_to_title[id] in output_lower:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # =========================================================\n",
        "    # Main pipeline\n",
        "    # =========================================================\n",
        "    def run_reflectrag(self, input_file: str, output_file: str, retriever=None):\n",
        "        fieldnames = [\n",
        "            \"id\", \"question\", \"correct_answer\", \"predicted_answer\",\n",
        "            \"is_answer_correct\", \"is_reference_correct\", \"doc_results\", \"reflection\", \"full_output\"\n",
        "        ]\n",
        "\n",
        "        processed_ids = set()\n",
        "        file_exists = os.path.exists(output_file) and os.path.getsize(output_file) > 0\n",
        "\n",
        "        if file_exists:\n",
        "            print(f\"Output file '{output_file}' found. Reading processed IDs for resuming...\")\n",
        "            with open(output_file, 'r', encoding='utf-8') as f_read:\n",
        "                reader = csv.DictReader(f_read)\n",
        "                for row in reader:\n",
        "                    if \"id\" in row and row[\"id\"]:\n",
        "                        processed_ids.add(row[\"id\"])\n",
        "            print(f\"-> Found {len(processed_ids)} processed items. Skipping them.\")\n",
        "        else:\n",
        "            print(f\"-> Starting new evaluation. Output will be saved to '{output_file}'.\")\n",
        "\n",
        "        ans_correct_total = 0\n",
        "        ref_correct_total = 0\n",
        "\n",
        "        with open(input_file, 'r', encoding='utf-8') as f_in, \\\n",
        "             open(output_file, 'a', newline='', encoding='utf-8') as f_out:\n",
        "\n",
        "            writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
        "            if not file_exists:\n",
        "                writer.writeheader()\n",
        "\n",
        "            count = 1 # For testing\n",
        "            for line_idx, line in enumerate(f_in):\n",
        "                #if count < 6: # For testing\n",
        "                #  count += 1\n",
        "                #else:\n",
        "                #  break\n",
        "                line = line.strip()\n",
        "                if not line: continue\n",
        "\n",
        "                try:\n",
        "                    row_data = json.loads(line)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Skipping line {line_idx+1}: Invalid JSON.\")\n",
        "                    continue\n",
        "\n",
        "                if \"canary\" in row_data: continue\n",
        "\n",
        "                curr_id = row_data.get(\"id\")\n",
        "\n",
        "                # --- 3. 跳過已處理題目 ---\n",
        "                if curr_id in processed_ids:\n",
        "                    continue\n",
        "\n",
        "                q_text, opt_str, correct_letter, _ = self.format_question_data(row_data)\n",
        "\n",
        "                # === retrieve documents ===\n",
        "                docs = []\n",
        "                if retriever:\n",
        "                    retrieved = retriever.invoke(q_text)\n",
        "                    docs = [d.page_content for d in retrieved]\n",
        "                else:\n",
        "                    docs = row_data.get(\"contexts\", [])  # fallback if no retriever\n",
        "\n",
        "                if not docs:\n",
        "                    print(f\"No documents found for {curr_id}, skipping.\")\n",
        "                    continue\n",
        "\n",
        "                #print('#'*60)\n",
        "                print(f\"#{count} Processing ID: {curr_id} ... \", end=\"\", flush=True)\n",
        "                count += 1\n",
        "\n",
        "                max_retries = 5\n",
        "                base_wait_time = 20\n",
        "\n",
        "                # === Stage 1 ===\n",
        "                doc_results = None\n",
        "                for attempt in range(max_retries + 1):\n",
        "                    try:\n",
        "                        doc_results = self.get_doc_confidences(q_text, docs)\n",
        "                        break  # 成功，跳出重試迴圈\n",
        "\n",
        "                    except Exception as e:\n",
        "                        if attempt < max_retries:\n",
        "                            # 計算等待時間: 20 * (2^0), 20 * (2^1)... -> 20, 40, 80, 160, 320\n",
        "                            wait_time = base_wait_time * (2 ** attempt)\n",
        "                            print(f\"\\n[API Error] Attempt {attempt+1}/{max_retries+1} failed: {e}\")\n",
        "                            print(f\"-> Retrying in {wait_time} seconds...\")\n",
        "                            time.sleep(wait_time)\n",
        "                        else:\n",
        "                            print(f\"\\n[API Failed] All {max_retries+1} attempts failed. Error: {e}\")\n",
        "                            doc_results = None # 標記為失敗\n",
        "\n",
        "                if doc_results is None:\n",
        "                    print(f\"Skipping ID {curr_id} due to API failures.\")\n",
        "                    continue\n",
        "\n",
        "                # === Stage 2 ===\n",
        "                reflection = None\n",
        "                for attempt in range(max_retries + 1):\n",
        "                    try:\n",
        "                        reflection = self.reflect_and_verify(q_text, doc_results, docs)\n",
        "                        break  # 成功，跳出重試迴圈\n",
        "\n",
        "                    except Exception as e:\n",
        "                        if attempt < max_retries:\n",
        "                            # 計算等待時間: 20 * (2^0), 20 * (2^1)... -> 20, 40, 80, 160, 320\n",
        "                            wait_time = base_wait_time * (2 ** attempt)\n",
        "                            print(f\"\\n[API Error] Attempt {attempt+1}/{max_retries+1} failed: {e}\")\n",
        "                            print(f\"-> Retrying in {wait_time} seconds...\")\n",
        "                            time.sleep(wait_time)\n",
        "                        else:\n",
        "                            print(f\"\\n[API Failed] All {max_retries+1} attempts failed. Error: {e}\")\n",
        "                            reflection = None # 標記為失敗\n",
        "\n",
        "                if reflection is None:\n",
        "                    print(f\"Skipping ID {curr_id} due to API failures.\")\n",
        "                    continue\n",
        "\n",
        "                # === Stage 3 ===\n",
        "                final_output = None\n",
        "                for attempt in range(max_retries + 1):\n",
        "                    try:\n",
        "                        final_output = self.generate_final_answer(q_text, opt_str, reflection)\n",
        "                        break  # 成功，跳出重試迴圈\n",
        "\n",
        "                    except Exception as e:\n",
        "                        if attempt < max_retries:\n",
        "                            # 計算等待時間: 20 * (2^0), 20 * (2^1)... -> 20, 40, 80, 160, 320\n",
        "                            wait_time = base_wait_time * (2 ** attempt)\n",
        "                            print(f\"\\n[API Error] Attempt {attempt+1}/{max_retries+1} failed: {e}\")\n",
        "                            print(f\"-> Retrying in {wait_time} seconds...\")\n",
        "                            time.sleep(wait_time)\n",
        "                        else:\n",
        "                            print(f\"\\n[API Failed] All {max_retries+1} attempts failed. Error: {e}\")\n",
        "                            final_output = None # 標記為失敗\n",
        "\n",
        "                if final_output is None:\n",
        "                    print(f\"Skipping ID {curr_id} due to API failures.\")\n",
        "                    continue\n",
        "\n",
        "                # === Evaluate correctness ===\n",
        "                predicted_ans = self.extract_model_answer(final_output)\n",
        "                is_ans_correct = (predicted_ans == correct_letter)\n",
        "\n",
        "                is_ref_correct = self.evaluate_references(final_output, curr_id)\n",
        "\n",
        "                ans_correct_total += is_ans_correct\n",
        "                ref_correct_total += is_ref_correct\n",
        "\n",
        "                print(f\"Ans: {predicted_ans} (Correct: {correct_letter}) | Ref Valid: {is_ref_correct}\")\n",
        "                #for doc in doc_results: # For check\n",
        "                #    print(f\"Doc {doc['doc_id']} | Confidence: {doc['confidence']}% | Answer: {doc['answer']}\")\n",
        "                #print(f\"\\nReflection: {reflection}\") # For check\n",
        "                #print(f\"\\nFinal output: {final_output}\") # For check\n",
        "                #print('#'*60, '\\n')\n",
        "\n",
        "                writer.writerow({\n",
        "                    \"id\": curr_id,\n",
        "                    \"question\": q_text,\n",
        "                    \"correct_answer\": correct_letter,\n",
        "                    \"predicted_answer\": predicted_ans,\n",
        "                    \"is_answer_correct\": is_ans_correct,\n",
        "                    \"is_reference_correct\": is_ref_correct,\n",
        "                    \"doc_results\": doc_results,\n",
        "                    \"reflection\": reflection,\n",
        "                    \"full_output\": final_output\n",
        "                })\n",
        "                f_out.flush()\n",
        "\n",
        "        print(\"Reflect-RAG evaluation complete.\")\n",
        "        print(f\"Correct Answers: {ans_correct_total}\")\n",
        "        print(f\"Correct References: {ref_correct_total}\")\n",
        "        print(f\"Accuracy: {ans_correct_total / line_idx * 100:.2f}%\")\n",
        "        print(f\"Reference Accuracy: {ref_correct_total / line_idx * 100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"litqa-v0.jsonl\"\n",
        "    output_file = \"reflectrag_eval.csv\"\n",
        "\n",
        "    if os.path.exists(output_file): # For testing\n",
        "        os.remove(output_file)\n",
        "\n",
        "    evaluator = ReflectRAGEvaluator(model_name=\"gemini-2.5-flash-lite\")\n",
        "    evaluator.run_reflectrag(input_file, output_file, retriever=retriever)"
      ],
      "metadata": {
        "id": "BjrAi2gKt8_Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}